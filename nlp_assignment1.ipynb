{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "29d5d834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', 'J', 'a', 'c', 'k', ' ', 'a', 'n', 'd', ' ', 'j', 'i', 'l', 'l', ' ', 'h', 'a', 'v', 'e', ' ', 'm', 'a', 'd', 'e', ' ', 'a', ' ', 'd', 'e', 'l', 'i', 'c', 'i', 'o', 'u', 's', ' ', ' ', ' ', ' ', ' ', ' ', ' ', 'd', 'i', 's', 'h', ' ', 'T', 'h', 'e', 'n', ' ', 't', 'h', 'e', 'y', ' ', 's', 't', 'a', 'r', 't', 'e', 'd', ' ', 't', 'o', ' ', 'p', 'l', 'a', 'y', ' ', 's', 'o', 'm', 'e', '', '', ' ', 'g', 'a', 'm', 'e', ' ', 'a', 'n', 'd', ' ', 'j', 'i', 'l', 'l', ' ', 'h', 'a', 's', ' ', 'a', 't', 't', 'a', 'h', 'a', 'c', 'd', ' ', 'a', ' ', 'p', 'h', 'o', 't', 'o', ' ', 'f', 'r', 'a', 'm', 'e', ' ', 't', 'o', ' ', 't', 'h', 'e', ' ', 's', 't', 'r', 'a', 'i', 'g', 'h', 't', '', ' ', 'w', 'a', 'l', 'l', ' ', 'a', 'n', 'd', ' ', 's', 'w', 'u', 'n', 'g', ' ', 'o', 'n', ' ', 's', 'e', 'a', 's', 'a', 'w', ' ', 'S', 'h', 'e', ' ', 'w', 'a', 's', ' ', 'v', 'e', 'r', 'y', ' ', 'h', 'a', 'p', 'p', 'y', ' ', 'A', 'f', 't', 'e', 'r', ' ', 't', 'h', 'e', ' ', 'g', 'a', 'm', 'e', ' ', 't', 'h', 'e', 'y', ' ', 'b', 'o', 't', 'h', ' ', 'w', 'e', 'n', 't', ' ', 't', 'o', ' ', 'c', 'e', 'n', 't', 'r', 'a', 'l', ' ', 'L', 'o', 'n', 'd', 'o', 'n', ' ', 't', 'o', ' ', 'e', 'n', 'j', 'o', 'y', ' ', 's', 'o', 'm', 'e', ' ', 'f', 'a', 's', 't', ' ', 'f', 'o', 'o', 'd']\n",
      "         Jack and jill have made a delicious       dish Then they started to play some game and jill has attahacd a photo frame to the straight wall and swung on seasaw She was very happy After the game they both went to central London to enjoy some fast food\n"
     ]
    }
   ],
   "source": [
    "string_list=\"         Jack and jill have made a delicious,       dish. Then they started to play some12 game! and jill has attahacd# [a] photo frame to the straight9 wall and swung on sea-saw. She was very happy. After the game, they both went to central London to enjoy some fast food.\"\n",
    "import string\n",
    "import re\n",
    "\n",
    "\n",
    "#function to remove punctuation and digits from sample string\n",
    "def remove_punction(text):\n",
    "    text_nonpunction = [re.sub('\\d',\"\",char) for char in text if char not in string.punctuation]\n",
    "    return text_nonpunction\n",
    "\n",
    "nonpun_string_list=remove_punction(string_list)\n",
    "print(nonpun_string_list)\n",
    "\n",
    "# a function to combine all elements\n",
    "def remove_punct_combine(text):\n",
    "    text_nonpunct = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    return text_nonpunct\n",
    "\n",
    "clean_string_list=remove_punct_combine(nonpun_string_list)\n",
    "\n",
    "print(clean_string_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b92a38e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\yogi\\anaconda3\\lib\\site-packages (3.6.5)\n",
      "Requirement already satisfied: click in c:\\users\\yogi\\anaconda3\\lib\\site-packages (from nltk) (8.0.3)\n",
      "Requirement already satisfied: joblib in c:\\users\\yogi\\anaconda3\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\yogi\\anaconda3\\lib\\site-packages (from nltk) (2021.8.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\yogi\\anaconda3\\lib\\site-packages (from nltk) (4.62.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\yogi\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.4)\n",
      "['         Jack and jill have made a delicious       dish Then they started to play some game and jill has attahacd a photo frame to the straight wall and swung on seasaw She was very happy After the game they both went to central London to enjoy some fast food']\n"
     ]
    }
   ],
   "source": [
    "#tokenize Sentence\n",
    "! pip install nltk\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "sents = sent_tokenize(clean_string_list)\n",
    "print(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83529e08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Jack', 'and', 'jill', 'have', 'made', 'a', 'delicious', 'dish', 'Then', 'they', 'started', 'to', 'play', 'some', 'game', 'and', 'jill', 'has', 'attahacd', 'a', 'photo', 'frame', 'to', 'the', 'straight', 'wall', 'and', 'swung', 'on', 'seasaw', 'She', 'was', 'very', 'happy', 'After', 'the', 'game', 'they', 'both', 'went', 'to', 'central', 'London', 'to', 'enjoy', 'some', 'fast', 'food']]\n"
     ]
    }
   ],
   "source": [
    "#tokenize words \n",
    "words = [word_tokenize(sent) for sent in sents]\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16c99d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Jack', 'jill', 'made', 'delicious', 'dish', 'Then', 'started', 'play', 'game', 'jill', 'attahacd', 'photo', 'frame', 'straight', 'wall', 'swung', 'seasaw', 'She', 'happy', 'After', 'game', 'went', 'central', 'London', 'enjoy', 'fast', 'food']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Yogi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#stopword removed \n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from string import punctuation\n",
    "customStopWords = set(stopwords.words('english')+list(punctuation))\n",
    "\n",
    "stopwords_list = [word for word in word_tokenize(clean_string_list) if word not in customStopWords]\n",
    "print(stopwords_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3ae2354",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['jack',\n",
       " 'jill',\n",
       " 'made',\n",
       " 'delici',\n",
       " 'dish',\n",
       " 'then',\n",
       " 'start',\n",
       " 'play',\n",
       " 'game',\n",
       " 'jill',\n",
       " 'attahacd',\n",
       " 'photo',\n",
       " 'frame',\n",
       " 'straight',\n",
       " 'wall',\n",
       " 'swung',\n",
       " 'seasaw',\n",
       " 'she',\n",
       " 'happi',\n",
       " 'after',\n",
       " 'game',\n",
       " 'went',\n",
       " 'central',\n",
       " 'london',\n",
       " 'enjoy',\n",
       " 'fast',\n",
       " 'food']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#stemming\n",
    "ps = nltk.PorterStemmer()\n",
    "def stemming(tokenized_text):\n",
    "    text =[ps.stem(word) for word in tokenized_text]\n",
    "    return text\n",
    "\n",
    "stemming(stopwords_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19c796ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Yogi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Jack', 'jill', 'made', 'delicious', 'dish', 'Then', 'started', 'play', 'game', 'jill', 'attahacd', 'photo', 'frame', 'straight', 'wall', 'swung', 'seasaw', 'She', 'happy', 'After', 'game', 'went', 'central', 'London', 'enjoy', 'fast', 'food']\n"
     ]
    }
   ],
   "source": [
    "#lemmatization\n",
    "nltk.download('wordnet')\n",
    "wn = nltk.WordNetLemmatizer()\n",
    "def lemmatizing(tokenized_text):\n",
    "    text = [wn.lemmatize(word) for word in tokenized_text]\n",
    "    return text\n",
    "\n",
    "final_string=lemmatizing(stopwords_list)\n",
    "print(final_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "58b6d647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              string\n",
      "0           Jack and jill have made a delicious, ...\n",
      "(1, 23)\n",
      "['', 'attahacd', 'central', 'delici', 'dish', 'enjoy', 'fast', 'food', 'frame', 'game', 'happi', 'jack', 'jill', 'london', 'made', 'photo', 'play', 'seasaw', 'start', 'straight', 'swung', 'wall', 'went']\n",
      "   0   1   2   3   4   5   6   7   8   9   ...  13  14  15  16  17  18  19  \\\n",
      "0   1   1   1   1   1   1   1   1   1   2  ...   1   1   1   1   1   1   1   \n",
      "\n",
      "   20  21  22  \n",
      "0   1   1   1  \n",
      "\n",
      "[1 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "#CountVectorizer\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "ps = nltk.PorterStemmer()\n",
    "\n",
    "#creating dataframe here of sample string\n",
    "df=pd.DataFrame({\"string\": [string_list]})\n",
    "print(df)\n",
    "def clean_text(text):\n",
    "    #removing digits here\n",
    "    text = \"\".join([re.sub('\\d',\"\",word.lower()) for word in text if word not in string.punctuation])\n",
    "    tokens = re.split('\\W+', text)\n",
    "    text1 = [ps.stem(word) for word in tokens if word not in stopwords]\n",
    "    return text1\n",
    "\n",
    "\n",
    "import sklearn\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "count_vect = CountVectorizer(analyzer=clean_text)\n",
    "\n",
    "\n",
    "X_counts = count_vect.fit_transform(df['string'])\n",
    "\n",
    "print(X_counts.shape)\n",
    "print(count_vect.get_feature_names())\n",
    "X_counts_df = pd.DataFrame(X_counts.toarray())\n",
    "print(X_counts_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2e04790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 45)\n",
      "['after the', 'and jill', 'and swung', 'attahacd photo', 'both went', 'central london', 'delicious dish', 'dish then', 'enjoy some', 'fast food', 'frame to', 'game and', 'game they', 'happy after', 'has attahacd', 'have made', 'jack and', 'jill has', 'jill have', 'london to', 'made delicious', 'on sea', 'photo frame', 'play some12', 'saw she', 'sea saw', 'she was', 'some fast', 'some12 game', 'started to', 'straight9 wall', 'swung on', 'the game', 'the straight9', 'then they', 'they both', 'they started', 'to central', 'to enjoy', 'to play', 'to the', 'very happy', 'wall and', 'was very', 'went to']\n",
      "   after the  and jill  and swung  attahacd photo  both went  central london  \\\n",
      "0          1         2          1               1          1               1   \n",
      "\n",
      "   delicious dish  dish then  enjoy some  fast food  ...  they both  \\\n",
      "0               1          1           1          1  ...          1   \n",
      "\n",
      "   they started  to central  to enjoy  to play  to the  very happy  wall and  \\\n",
      "0             1           1         1        1       1           1         1   \n",
      "\n",
      "   was very  went to  \n",
      "0         1        1  \n",
      "\n",
      "[1 rows x 45 columns]\n"
     ]
    }
   ],
   "source": [
    "#ngram(2,2)\n",
    "ngram_vect = CountVectorizer(ngram_range=(2,2))\n",
    "X_counts = ngram_vect.fit_transform(df['string'])\n",
    "print(X_counts.shape)\n",
    "print(ngram_vect.get_feature_names())\n",
    "X_counts_df = pd.DataFrame(X_counts.toarray())\n",
    "X_counts_df.columns = ngram_vect.get_feature_names()\n",
    "print(X_counts_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b02c06a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 90)\n",
      "['after the', 'after the game', 'and jill', 'and jill has', 'and jill have', 'and swung', 'and swung on', 'attahacd photo', 'attahacd photo frame', 'both went', 'both went to', 'central london', 'central london to', 'delicious dish', 'delicious dish then', 'dish then', 'dish then they', 'enjoy some', 'enjoy some fast', 'fast food', 'frame to', 'frame to the', 'game and', 'game and jill', 'game they', 'game they both', 'happy after', 'happy after the', 'has attahacd', 'has attahacd photo', 'have made', 'have made delicious', 'jack and', 'jack and jill', 'jill has', 'jill has attahacd', 'jill have', 'jill have made', 'london to', 'london to enjoy', 'made delicious', 'made delicious dish', 'on sea', 'on sea saw', 'photo frame', 'photo frame to', 'play some12', 'play some12 game', 'saw she', 'saw she was', 'sea saw', 'sea saw she', 'she was', 'she was very', 'some fast', 'some fast food', 'some12 game', 'some12 game and', 'started to', 'started to play', 'straight9 wall', 'straight9 wall and', 'swung on', 'swung on sea', 'the game', 'the game they', 'the straight9', 'the straight9 wall', 'then they', 'then they started', 'they both', 'they both went', 'they started', 'they started to', 'to central', 'to central london', 'to enjoy', 'to enjoy some', 'to play', 'to play some12', 'to the', 'to the straight9', 'very happy', 'very happy after', 'wall and', 'wall and swung', 'was very', 'was very happy', 'went to', 'went to central']\n",
      "   after the  after the game  and jill  and jill has  and jill have  \\\n",
      "0          1               1         2             1              1   \n",
      "\n",
      "   and swung  and swung on  attahacd photo  attahacd photo frame  both went  \\\n",
      "0          1             1               1                     1          1   \n",
      "\n",
      "   ...  to the  to the straight9  very happy  very happy after  wall and  \\\n",
      "0  ...       1                 1           1                 1         1   \n",
      "\n",
      "   wall and swung  was very  was very happy  went to  went to central  \n",
      "0               1         1               1        1                1  \n",
      "\n",
      "[1 rows x 90 columns]\n"
     ]
    }
   ],
   "source": [
    "#ngram(2,3)\n",
    "ngram_vect = CountVectorizer(ngram_range=(2,3))\n",
    "X_counts = ngram_vect.fit_transform(df['string'])\n",
    "print(X_counts.shape)\n",
    "print(ngram_vect.get_feature_names())\n",
    "X_counts_df = pd.DataFrame(X_counts.toarray())\n",
    "X_counts_df.columns = ngram_vect.get_feature_names()\n",
    "print(X_counts_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5bf3363a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 23)\n",
      "['', 'attahacd', 'central', 'delici', 'dish', 'enjoy', 'fast', 'food', 'frame', 'game', 'happi', 'jack', 'jill', 'london', 'made', 'photo', 'play', 'seasaw', 'start', 'straight', 'swung', 'wall', 'went']\n",
      "             attahacd   central    delici      dish     enjoy      fast  \\\n",
      "0  0.185695  0.185695  0.185695  0.185695  0.185695  0.185695  0.185695   \n",
      "\n",
      "       food     frame      game  ...    london      made     photo      play  \\\n",
      "0  0.185695  0.185695  0.371391  ...  0.185695  0.185695  0.185695  0.185695   \n",
      "\n",
      "     seasaw     start  straight     swung      wall      went  \n",
      "0  0.185695  0.185695  0.185695  0.185695  0.185695  0.185695  \n",
      "\n",
      "[1 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "#TF-IDF\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vect = TfidfVectorizer(analyzer=clean_text)\n",
    "X_tfidf = tfidf_vect.fit_transform(df['string'])\n",
    "print(X_tfidf.shape)\n",
    "print(tfidf_vect.get_feature_names())\n",
    "X_tfidf_df = pd.DataFrame(X_tfidf.toarray())\n",
    "X_tfidf_df.columns = tfidf_vect.get_feature_names()\n",
    "print(X_tfidf_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870ba3e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
